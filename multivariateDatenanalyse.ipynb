{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivaraite Datenanalyse\n",
    "# Lineare Diskriminanzanalyse (LDA)\n",
    "### Michael Araz, Daniel Hasenklever, Stefan Pede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mmPfad = r\"D:\\C\\Uni\\Master\\KSS\\MV_Analyse\\Messmatrix.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenschaften der Messmatrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(mmPfad)\n",
    "print(\"Anzahl der Kennwerte: \"+str(df.shape[1]))\n",
    "print(\"Anzahl der vermessenen Rohre: \"+str(df.shape[0]))\n",
    "print(\"Anzahl der gefahrenen Produkte: \"+str(df.groupby([\"Header_Leitguete\",\"Header_Soll_AD\",\"Header_Soll_WD\"])[\"Header_Pseudonummer\"].agg([\"count\"]).shape[0]))\n",
    "print(\"Anzahl der Walzlose: \"+str(len(pd.unique(df[\"Header_Walzlos\"]))))\n",
    "print(\"\\nAuszug:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produkte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupby([\"Header_Leitguete\",\"Header_Soll_AD\",\"Header_Soll_WD\"])[\"Header_Pseudonummer\"].agg([\"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorverarbeiten der Kennwerte\n",
    "Entfernen derjenigen Spalten und Zeilen mit zu vielen ungültigen/fehlenden Einträgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processNaNs(df, bounds):\n",
    "## df: pandas dataframe that is to be processed\n",
    "## bounds: list of percentages for column and row filtering, e.g. [15, 5]\n",
    "    relNaNsCol = np.sum(np.isnan(df))/df.shape[0]*100\n",
    "    # schmeiße zunächst alle Spalten heraus, die mehr als bestimmte Prozent an NaNs haben\n",
    "    spaltenSchranke = bounds[0] # % der NaNs in Spalte\n",
    "    keep = [i for i in np.arange(len(relNaNsCol)) if relNaNsCol[i] <= spaltenSchranke]\n",
    "    dfVV = df[df.columns[keep]] # extrahiere Spalten\n",
    "\n",
    "    # gleiches auf Zeilen anwenden\n",
    "    zeilenSchranke = bounds[1] # % der NaNs in Zeile\n",
    "    relNaNsRow = dfVV.isnull().sum(axis=1)/dfVV.shape[1]*100\n",
    "    keep = [i for i in np.arange(len(relNaNsRow)) if relNaNsRow[i] <= zeilenSchranke]\n",
    "    dfVV2 = dfVV.iloc[keep] #extraheire Zeilen\n",
    "\n",
    "    #übrige NaNs mit Mittelwert aus Spalten auffüllen\n",
    "    dfVV2 = dfVV2.fillna(dfVV2.mean())\n",
    "\n",
    "    # Ausgabe\n",
    "    print(\"Daten nach Vorverarbeitung:\")\n",
    "    print(\"Anzahl der Kennwerte: \"+str(dfVV2.shape[1]))\n",
    "    print(\"Anzahl der vermessenen Rohre: \"+str(dfVV2.shape[0]))\n",
    "    print(\"Anzahl der gefahrenen Produkte: \"+str(dfVV2.groupby([\"Header_Leitguete\",\"Header_Soll_AD\",\"Header_Soll_WD\"])[\"Header_Pseudonummer\"].agg([\"count\"]).shape[0]))\n",
    "    print(\"Anzahl der Walzlose: \"+str(len(pd.unique(dfVV2[\"Header_Walzlos\"]))))\n",
    "\n",
    "    return dfVV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def labelData(pdDaFr, label):\n",
    "# Diese Funktion erhält als Eingabegrößen das vorverarbeitete Dataframe und den Namen der Header-Spalte, die als Label\n",
    "# dienen soll, mittelwertfreie Rückgabe ohne die Header-Informationen\n",
    "    pdDaFr = pdDaFr.copy()\n",
    "    anzHeaderSpalten = 6\n",
    "    preLabels = pd.unique(pdDaFr[label])\n",
    "    postLabels = np.arange(len(preLabels))\n",
    "\n",
    "    # kopiere Spalte mit Labels\n",
    "    class_label = pdDaFr[label].values\n",
    "    # kopiere nicht den Header\n",
    "    pdDaFr = pdDaFr[pdDaFr.columns[6:]]\n",
    "    pdDaFr = pdDaFr-pdDaFr.mean()\n",
    "    # ersetze die Labels durch Integer-Labels\n",
    "    for i in postLabels:\n",
    "        class_label[class_label == preLabels[i]] = postLabels[i]\n",
    "    #passe Datentyp an\n",
    "    class_label = class_label.astype(np.int64)\n",
    "    \n",
    "    return(pdDaFr, class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocess data and label it\n",
    "dfVV2 = processNaNs(df, [15,5])\n",
    "label = \"Header_Walzlos\" #als Beispiel\n",
    "data_preProc_LDA, labels = labelData(dfVV2, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTrainingAndTestData(data, labels, fraction, minNoTubesPerSet):\n",
    "## split data into training- and test-data\n",
    "## data: pandas dataframe that is to be splitted\n",
    "## labels: label for each row of the dataframe\n",
    "## fraction: e.g. 5 to get a fifth, i.e. 20%, of the original data as test data\n",
    "## minNoTubesPerSet: minimum number of tubes in a set to consider it, e.g. 20\n",
    "\n",
    "    training_set = pd.DataFrame()\n",
    "    test_set = pd.DataFrame()\n",
    "    training_labels=np.array([])\n",
    "    test_labels = np.array([])\n",
    "    \n",
    "    for i in np.unique(labels):\n",
    "        anz_Rohr_in_Walzlos = np.sum(labels==i)\n",
    "        if anz_Rohr_in_Walzlos > minNoTubesPerSet:\n",
    "            test_data_size = int(anz_Rohr_in_Walzlos/fraction)\n",
    "            training_data_size = anz_Rohr_in_Walzlos-test_data_size\n",
    "            \n",
    "            tmp=data[labels==i]\n",
    "            training_set=training_set.append(tmp[:training_data_size])\n",
    "            test_set = test_set.append(tmp[training_data_size:])\n",
    "\n",
    "            training_labels = np.concatenate((training_labels,i*np.ones(training_data_size)))\n",
    "            test_labels = np.concatenate((test_labels,i*np.ones(test_data_size)))\n",
    "    \n",
    "    return ([training_set, test_set],[training_labels, test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split data into test and training data\n",
    "sets, sets_labels = getTrainingAndTestData(data_preProc_LDA, labels, 5, 30)\n",
    "training_set = sets[0]\n",
    "test_set = sets[1]\n",
    "training_labels = sets_labels[0]\n",
    "test_labels = sets_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dropCorrelatedColumns(sets, covBound):\n",
    "    from scipy import stats\n",
    "\n",
    "    ## suche hohe Werte in Kovarianz-Matrix\n",
    "    if covBound < 0:\n",
    "        text = \"korrelieren negativ\"\n",
    "        kovBool = pd.DataFrame(np.cov((stats.zscore(sets[0]).T)) < covBound)\n",
    "    else:\n",
    "        text = \"korrelieren positiv\"\n",
    "        kovBool = pd.DataFrame(np.cov((stats.zscore(sets[0]).T)) > covBound)        \n",
    "    ## suche diejenigen, die nicht auf Diagonale liegen\n",
    "    korr = []\n",
    "    for a,b in zip(np.where(kovBool)[0], np.where(kovBool)[1]):\n",
    "        if (a != b):\n",
    "            korr.append([a,b])\n",
    "    print(korr)\n",
    "    ## sortiere diese und finde einzigartige\n",
    "    korr = [sorted(i) for i in korr]\n",
    "    korrWD = []\n",
    "    for i in korr:\n",
    "        if i not in korrWD:\n",
    "            korrWD.append(i)\n",
    "            print(sets[0].columns[i[0]],\" und \",sets[1].columns[i[1]],text)\n",
    "    ## erhalte Indizes der korrelierenden Spalten\n",
    "    drop = []\n",
    "    for i in korrWD:\n",
    "        drop.append(i[1])\n",
    "\n",
    "    ## lösche diese aus den vorliegenden Sets           \n",
    "    retTrainingSet = sets[0].drop(sets[0].columns[drop], axis=1)\n",
    "    retTestSet = sets[1].drop(sets[1].columns[drop], axis=1)\n",
    "    return(retTrainingSet, retTestSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## drop correlated columns\n",
    "## LDA zeigt Warnung, dass Variablen korrelieren bis zu einer Kovarianz von 0.9 an\n",
    "training_set_noCorr, test_set_noCorr = dropCorrelatedColumns([training_set, test_set], 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduktion der Daten um korrelierende Eingangsgrößen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train classificator\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "sklearn_lda = LDA(n_components=10)\n",
    "data_lda = sklearn_lda.fit_transform(training_set_noCorr, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test\n",
    "# with test-data\n",
    "prediction = sklearn_lda.predict(test_set_noCorr)\n",
    "print(\"Test-Set-Prediction: \",np.mean(prediction == test_labels))\n",
    "#with training-data\n",
    "pred = sklearn_lda.predict(training_set_noCorr)\n",
    "print(\"Training-Set-Prediction: \",np.mean(pred == training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STRANGE\n",
    "## die Genauigkeit des Klassifikators ändert sich nicht für verschiedene n_components\n",
    "## sklearn_lda.coef_ enthält 2D statt 1D-Daten\n",
    "## kann es sein, dass für jedes Walzlos ein eigener Klassifikator erstellt wird oder so ähnlich?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
